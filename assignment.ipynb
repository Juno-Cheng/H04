{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc28798-22d3-41fb-9bd2-826bd266bdf2",
   "metadata": {},
   "source": [
    "# Hands-On Assignment 4\n",
    "\n",
    "In this assignment, we will introduce you to [scikit-learn](https://scikit-learn.org/stable/index.html) (imported in Python as `sklearn`),\n",
    "the most ubiquitous machine learning library for Python.\n",
    "We will use this library to train and interpret a few standard models in machine learning.\n",
    "Finally, we will recreate a few classic observations in machine learning.\n",
    "\n",
    "scikit-learn is a large library that contains tools for classification, regression, clustering, feature selection, evaluation, and preprocessing.\n",
    "We will also refer to \"scikit-learn\" as \"sklearn\".\n",
    "Since Python doesn't like package names with a dash, `sklearn` is what you use to import the library in Python.\n",
    "It uses a fairly consistent API that lets you swap out components without needing the change your entire infrastructure,\n",
    "e.g., you can swap out the type of classifier you use without needing to change the rest of your code.\n",
    "It is so ubiquitous in machine learning that other machine learning libraries will often use the same terminology and methods (which we will discuss later).\n",
    "In this assignment, we will cover the basics that you need to know to work with sklearn,\n",
    "but there are many additional resources on the web.\n",
    "If you need more instruction, we recommend this [text tutorial](https://scikit-learn.org/stable/getting_started.html)\n",
    "and this [video tutorial](https://www.youtube.com/watch?v=0B5eIE_1vpU).\n",
    "sklearn also has a very complete [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "and many curated [examples](https://scikit-learn.org/stable/auto_examples/index.html).\n",
    "\n",
    "The objective of this assignment is for you to learn about:\n",
    " - The basic functionality and terminology of sklearn.\n",
    " - How to visualize and interpret decision boundaries.\n",
    " - How to use simple and effective machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4896430e-0a5d-471c-9c1f-f03a345e8951",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Revisiting Synthetic Covid-19 Data\n",
    "\n",
    "For this assignment, we will revisit the same data domain we used in HO1 and HO2: the synthetic Covid-19 data.\n",
    "Except this time we will have access to more information about the patients.\n",
    "Our task will remain the same, predicting whether a patient has Covid-19.\n",
    "\n",
    "Load this dataset, included in this repository as `synthetic_covid_data_v2.csv`, using Python, and feel free to explore its content.\n",
    "Note again that this data is entirely synthetic (because in general, an individual's health care data is strictly regulated).\n",
    "There may be unrealistic qualities of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e629c-407a-41ca-a7d0-45557c91238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import pandas\n",
    "import sklearn.base\n",
    "import sklearn.datasets\n",
    "import sklearn.inspection\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.neighbors\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.tree\n",
    "\n",
    "# Read data from the given csv file into a Pandas dataframe.\n",
    "covid_data = pandas.read_csv(\"synthetic_covid_data_v2.csv\", index_col = 0)\n",
    "\n",
    "# Print column and type information.\n",
    "covid_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac71d5-3ca0-4f64-b441-8149e3af2eb5",
   "metadata": {},
   "source": [
    "First, note that the column 'sex' is encoded as an object, rather than a number.\n",
    "Upon inspection, we find that this column takes on either 'male' or 'female' values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7551095-6286-4769-8d44-6e3da03da2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first few rows.\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45e70b-7816-4c79-940c-fe6ef3f66045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect any numeric columns.\n",
    "covid_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce5e6c-2a3b-41ac-81c8-33a77498ab96",
   "metadata": {},
   "source": [
    "There is a non-numeric column (\"sex\") that we need to prepare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded27b0d-c6d3-4e3b-a38c-fd837d5cb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which categories of sex are in the dataset.\n",
    "print(\"Possible values for the sex column: \", sorted(set(covid_data[\"sex\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe2ba5-f6ce-4b02-bbae-47d8ac751260",
   "metadata": {},
   "source": [
    "Let's one-hot encode the \"sex\" column, just like we did in HO3.\n",
    "A useful function to use here is [pandas.get_dummies()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html),\n",
    "which can handle basic one-hot encoding for us\n",
    "(in complex situations you may have to do your own encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11451a56-c206-4071-8059-0367089098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_sex = pandas.get_dummies(covid_data['sex'], prefix = 'sex')\n",
    "one_hot_sex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e181127-7a7e-48e6-8184-8295af8ca14a",
   "metadata": {},
   "source": [
    "Now just add it to our main frame and remove the old \"sex\" column.\n",
    "Remember, the `axis = 1` is so that we can add columns (`axis = 1`) instead of rows (`axis = 0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9917d6fa-3861-4d6a-a357-e1ce2fbd9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data = pandas.concat([covid_data, one_hot_sex], axis = 1)\n",
    "covid_data.pop('sex')\n",
    "covid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b1f8df-d1e8-42a1-a0c8-8478e89259f8",
   "metadata": {},
   "source": [
    "Now that our data is clean, we can separate out the features and labels.\n",
    "For features, we are going to focus on the new columns added for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9580a-0c58-429c-acb8-9cf43d210872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels.\n",
    "covid_labels = covid_data['infected']\n",
    "covid_features = covid_data[['titer', 'age', 'height', 'weight', 'blood_oxygen',\n",
    "                             'sex_female', 'sex_male', 'sex_other', 'sex_prefer not to say']]\n",
    "\n",
    "covid_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfef362-6039-43fc-9f45-888e443e2f0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: Introducing scikit-learn\n",
    "\n",
    "scikit-learn (sklearn) is probably the most popular machine learning library for Python.\n",
    "Therefore, it's the go-to choice for us to use in the course.\n",
    "In this section, we will introduce you to some of the functionality we will be using from sklearn.\n",
    "\n",
    "This section will cover using sklearn for:\n",
    " - preprocessing data\n",
    " - training and using models\n",
    " - visualizing model outputs\n",
    " - combining processing steps into pipelines\n",
    "\n",
    "Here are some links you may find useful:\n",
    " - [scikit-learn Homepage](https://scikit-learn.org/stable/index.html)\n",
    " - [API Documentation](https://scikit-learn.org/stable/modules/classes.html)\n",
    " - [Official User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    " - [Official Examples](https://scikit-learn.org/stable/auto_examples/index.html)\n",
    " - [Recommended Text Tutorial](https://scikit-learn.org/stable/getting_started.html)\n",
    " - [Recommended Video Tutorial](https://www.youtube.com/watch?v=0B5eIE_1vpU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd8bf2-d872-46aa-b4aa-26a0dffc657e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Even if your data is immaculate and you have already done all the data cleaning steps we saw in HO3,\n",
    "there are still a few standard transformations that we routinely apply on data to make it more suited for machine learning.\n",
    "On of those transformations is *feature scaling*.\n",
    "\n",
    "[Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) tries to account for different ranges of numbers that occur in different columns.\n",
    "To see what we mean, let's take a look at our Covid-19 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e2fb7-23e3-40fd-862c-ea0d1daeb5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43272681-7855-4f65-a506-98072a2c2252",
   "metadata": {},
   "source": [
    "We can see our \"blood_oxygen\" column has values in the range of $[0.80, 1.00]$,\n",
    "but our \"height\" column goes from $144$ to $213$.\n",
    "All of these values makes sense in the context of their own column,\n",
    "but it can be hard for our machine learning models to understand that just because \"blood_oxygen\" is $ 1 / 200 $ the value of \"height\" does not mean it is less important.\n",
    "\n",
    "To overcome this issue, we can use feature scaling.\n",
    "\n",
    "There are several different [feature scaling methods available in sklearn](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)\n",
    "(and you can always implement your own).\n",
    "It's important that you consider what method of scaling works best for your specific domain.\n",
    "Two of the most popular feature scaling methods are:\n",
    " - [Min-Max scaling](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)),\n",
    "    which makes sure that all values are scaled between $0.0$ and $1.0$.\n",
    " - [Z-score Normalization](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)) also confusingly called \"Standardization\",\n",
    "     which makes the new mean of the data $0.0$ and the variance $1.0$.\n",
    "\n",
    "The below image shows an example of a two-dimensional dataset being scaled.\n",
    "On the left, the unscaled features make it look like the X-axis is the only feature that actually matters.\n",
    "But after being scaled, on the right, we can see that we do actually need to use the Y-axis to properly classify the data.\n",
    "Aside from making the features easier for our machine learning methods to interpret,\n",
    "feature scaling also makes it much easier for us to visualize and interpret the data.\n",
    "\n",
    "<center><img src=\"sklearn_scaling.png\" width=\"750px\"/></center>\n",
    "<center style='font-size: small'>Image from <a href='https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html'>scikit-learn</a>.</center>\n",
    "\n",
    "Feature scalers in sklearn are in the family referred to as [transformers](https://scikit-learn.org/stable/glossary.html#term-transformers).\n",
    "Transformers all have a `fit_transform()` method (sometimes broken out into individual `fit()` and `transform()` methods).\n",
    "As the name suggests, these classes are all meant for transforming data (usually in preparation for feeding it into a machine learning model).\n",
    "\n",
    "As a concrete example, look at the [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) class.\n",
    "It has both a `fit_transform()` and `fit()` & `transform()` methods.\n",
    "The `fit()` method computes the min and max values of the passed in data,\n",
    "and the `transform()` method then uses those min/max values to do the actual transformation.\n",
    "The `fit_transform()` method just combines both steps into one method.\n",
    "In this simple case, it may seem unnecessary to have both `fit()` and `transform()` methods,\n",
    "but in more complex cases we may be using other machine learning models to clean to data for our future machine learning models.\n",
    "In that case, the fitting would be much more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d192f95-b511-49de-a736-fbf4697d2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "data = [[1.0], [2.0], [3.0]]\n",
    "print(\"Min-Max scaling with data: \", data)\n",
    "scaler.fit(data)\n",
    "print(scaler.transform(data))\n",
    "\n",
    "# We can also use fit_transform().\n",
    "data = [[100], [200], [300]]\n",
    "print(\"Min-Max scaling with data: \", data)\n",
    "print(scaler.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d6c45-8a73-4a3b-aede-c99f57f79476",
   "metadata": {},
   "source": [
    "Notice how the two different sets of numbers (which are very different in terms of magnitude),\n",
    "have the same scaled values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8936cb-b921-4d34-922c-1467c9090cb0",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange\";>★ Task 1.A</h3>\n",
    "\n",
    "Complete the below function which takes a frame of features and a list of columns to work on.\n",
    "The function should scale the passed in features (using the specified columns)\n",
    "using a sklearn [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "The function should return a frame with the scaled data instead of the original data\n",
    "(column ordering does not matter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962d996-70e5-4e1f-a5c3-f032f193c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(features, columns):\n",
    "    \"\"\"\n",
    "    Scale numeric columns.\n",
    "\n",
    "    Args:\n",
    "        features: A pandas DataFrame.\n",
    "        columns: A list of column names to scale.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with scaled versions of the specified columns.\n",
    "    \"\"\"\n",
    "\n",
    "    return NotImplemented\n",
    "\n",
    "print(\"Scaled Covid-19 features:\")\n",
    "scaled_features = scale_data(covid_features, ['titer', 'age', 'height', 'weight', 'blood_oxygen'])\n",
    "if (not isinstance(scaled_features, type(NotImplemented))):\n",
    "    covid_features = scaled_features\n",
    "\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d6c82-05d0-49da-bd0c-637b44fe8679",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Predictor API\n",
    "\n",
    "In the last section, we used feature scalers which sklearn classifies as \"transformers\".\n",
    "Now, we will looks at sklearn [predictors](https://scikit-learn.org/stable/glossary.html#term-predictors).\n",
    "Predictors all have `fit()` and `predict()` methods (sometimes combined into a `fit_predict()` method).\n",
    "Predictors encapsulate our core machine learning models: classifiers, regressors, and clusterers.\n",
    "([classifiers](https://scikit-learn.org/stable/glossary.html#term-classifiers) and [regressors](https://scikit-learn.org/stable/glossary.html#term-regressors)\n",
    "will also have a `score()` method that will give you a basic numeric score.)\n",
    "\n",
    "To test out this interface, let's create a [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) classifier.\n",
    "In the below cell, you can see that we construct a perceptron classifier.\n",
    "Then, we can call `fit()` on it to fit/train the classifier with our features and labels\n",
    "(note that we are just playing around with the sklearn API and have not yet split our data into train and test data).\n",
    "After our classifier is fitted/trained, we can ask it for predictions with the `predict()` method.\n",
    "We can also use the `score()` method to get an idea of how well the classifier is doing.\n",
    "In HO2, we discussed different evaluation methods and how important it is to find the proper metric for your problem.\n",
    "But as we are just exploring the sklearn API, the generic score method (which returns accuracy for classifiers) will work for now.\n",
    "\n",
    "A question to ask at this point is: \"Hey, why did you choose to use a perceptron classifier in this case?\".\n",
    "The answer to that is: \"Good question, no real reason, it just works.\"\n",
    "This exchange may be a bit silly, but highlights one of sklearn's biggest strengths: its flexibility.\n",
    "We are currently using a perceptron classifier,\n",
    "but we can easily swap out the classifier we are using with only changing the line constructing our classifier (and whatever imports we need).\n",
    "In the code below, try swapping out the perceptron classifier with other classifiers such as\n",
    "[K-Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "or [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)\n",
    "and see that the code still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453402bd-d103-4939-a7fd-8cad06737c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier instance.\n",
    "classifier = sklearn.linear_model.Perceptron()\n",
    "\n",
    "# Train/fit the classifier.\n",
    "classifier.fit(covid_features, covid_labels)\n",
    "\n",
    "# Look at the first 20 predictions.\n",
    "print(\"First 20 predictions:\")\n",
    "print(classifier.predict(covid_features)[0:20])\n",
    "\n",
    "# Get a score for the classifier (for classifiers, this score is accuracy).\n",
    "print(\"Classifier's score:\", classifier.score(covid_features, covid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba65edff-5225-4428-9af7-523ed93a3c50",
   "metadata": {},
   "source": [
    "## Visualization - Decision Boundaries\n",
    "\n",
    "As you may have already discussed, many classifiers work by creating a [decision boundary](https://en.wikipedia.org/wiki/Decision_boundary).\n",
    "Decision boundaries are a line (or curve, plane, etc) that binary classifiers use to split up a feature space.\n",
    "Data points on one side of the decision boundary are assigned one class, and the other points are assigned the other class.\n",
    "\n",
    "Visualizing a classifier's decision boundary can help us understand the choices that a classifier is making.\n",
    "Unfortunately when visualizing decision boundaries we are limited to two or three dimensions,\n",
    "but that can still be enough to help us understand what our classifier is doing.\n",
    "\n",
    "sklearn provides some tools like [DecisionBoundaryDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html)\n",
    "to help visualize decision boundaries.\n",
    "Below is a function `visualize_decision_boundary()` that let's us see a decision boundary of a trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d8cb5-a0cd-45f5-bb8d-cad37bd73df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_SIZE = 5\n",
    "FIGURE_RESOLUTION = 500\n",
    "FIGURE_COLORMAP = 'coolwarm'\n",
    "\n",
    "def visualize_decision_boundary(classifier, features, labels, title = None):\n",
    "    \"\"\"\n",
    "    Visualize the decision boundary of a trained binary classifier\n",
    "    using the FIRST TWO columns of the passed in features.\n",
    "    \"\"\"\n",
    "\n",
    "    figure, axis = matplotlib.pyplot.subplots(1, 1, figsize = (FIGURE_SIZE, FIGURE_SIZE))\n",
    "    axis.set_title(title)\n",
    "\n",
    "    # Draw the decision boundary.\n",
    "    decision_boundary = sklearn.inspection.DecisionBoundaryDisplay.from_estimator(\n",
    "        classifier, features,\n",
    "        response_method = \"predict\", ax = axis,\n",
    "        xlabel = features.columns[0], ylabel = features.columns[1],\n",
    "        cmap = FIGURE_COLORMAP, grid_resolution = FIGURE_RESOLUTION\n",
    "    )\n",
    "\n",
    "    # Also display the data points.\n",
    "    axis.scatter(\n",
    "        features[features.columns[0]], features[features.columns[1]],\n",
    "        c = labels, cmap = FIGURE_COLORMAP, alpha = 0.5,\n",
    "    )\n",
    "\n",
    "    # Add a legend for the colors.\n",
    "    colorbar = figure.colorbar(decision_boundary.surface_, ax = axis, ticks = [0, 1])\n",
    "    colorbar.ax.set_yticklabels([\"negative\", \"positive\"])\n",
    "\n",
    "    return axis\n",
    "\n",
    "# Make a classifier we want to use.\n",
    "classifier = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "# Pick the two features we want to see.\n",
    "two_features = covid_features[['titer', 'blood_oxygen']]\n",
    "\n",
    "classifier.fit(two_features, covid_labels)\n",
    "visualize_decision_boundary(classifier, two_features, covid_labels, \"Titer vs Blood Oxygen\")\n",
    "\n",
    "print(\"Accuracy: \", classifier.score(two_features, covid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f59faa8-2d69-4369-b0ac-8db17d608aca",
   "metadata": {},
   "source": [
    "Notice how the decision boundary does a decent job in separating the data,\n",
    "but still misses several points.\n",
    "Every blue point in red space (and vice versa) is a misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148eeacb-2f52-46b8-99fd-e9608f1dc7f8",
   "metadata": {},
   "source": [
    "### An Aside on Binary vs Multiclass Classifiers\n",
    "\n",
    "Note that we have only been discussing [binary classifiers](https://en.wikipedia.org/wiki/Binary_classification),\n",
    "i.e., classifiers that predict False/True (0/1).\n",
    "But, any binary classifier can be easily [converted to a multiclass classifier](https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_binary).\n",
    "For example if we had three classes (A, B, and C) and we wanted to predict them using only a binary classifier,\n",
    "then we could first train a binary classifier that predicts if a data point is A vs (B or C),\n",
    "then we can train another binary classifier that predicts B vs (A or C),\n",
    "and one more classifier that is C vs (A or B).\n",
    "Then for each data point, we can choose the label that the classifiers are most sure about.\n",
    "This method is called the one-vs-rest or one-vs-all technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668a523-f2ed-4de9-9095-b09a9335cc31",
   "metadata": {},
   "source": [
    "## The Machine Learning Pipeline\n",
    "\n",
    "Throughout our different assignments,\n",
    "we often modify/process our data in different ways and then store the data in some variable (or the same variable)\n",
    "so that we can use it in a future cell.\n",
    "This works well for these types of assignments where we want to explore our data,\n",
    "but what about when we want to use our data for actual work\n",
    "(not just code that is in an iPython notebook)?\n",
    "In these cases, we may want to use a [sklearn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "(The concept of machine learning pipelines exists in most machine learning libraries, not just sklearn.)\n",
    "\n",
    "A sklearn pipeline is just a series of sklearn transformers and a final sklearn estimator.\n",
    "This means that a pipeline is just a list of objects that has `fit_transform()` methods,\n",
    "and ends with a predictor that has `fit()`/`predict()` methods.\n",
    "The pipeline itself is also a predictor, so will use the same `fit()`/`predict()` methods.\n",
    "When you call `fit()` on a pipeline,\n",
    "the `fit_transform()` method of all the transformers is called followed by the predictor's `fit()` method.\n",
    "When you call `predict()` on a pipeline,\n",
    "the `fit_transform()` method of all the transformers is called followed by the predictor's `predict()` method.\n",
    "\n",
    "For example, we can make a pipeline that uses two dummy transformers (`some_transformer_1` and `some_transformer_2`)\n",
    "and a dummy classifier (`some_classifier`):\n",
    "```\n",
    "steps = [\n",
    "    ('Do Something', some_transformer_1),\n",
    "    ('Do Something Else', some_transformer_2),\n",
    "    ('Make a Prediction', some_classifier),\n",
    "]\n",
    "pipeline = sklearn.pipeline.Pipeline(steps)\n",
    "\n",
    "pipeline.fit(train_features, train_labels)\n",
    "predictions = pipeline.predict(test_features)\n",
    "```\n",
    "\n",
    "The above pipeline gives the exact same result as the code below:\n",
    "```\n",
    "# Transform the training data and fit the classifier.\n",
    "transformed_train_data_1 = some_transformer_1.fit_transform(train_features)\n",
    "transformed_train_data_2 = some_transformer_2.fit_transform(transformed_train_data_1)\n",
    "some_classifier.fit(transformed_train_data_2, train_labels)\n",
    "\n",
    "# Transform the test data and make predictions.\n",
    "transformed_test_data_1 = some_transformer_1.fit_transform(test_features)\n",
    "transformed_test_data_2 = some_transformer_2.fit_transform(transformed_test_data_1)\n",
    "predictions = some_classifier.fit(transformed_test_data_2)\n",
    "```\n",
    "\n",
    "The nice thing about pipelines is that are also estimators,\n",
    "so you can pass them to any method that expects an estimator/predictor/classifier.\n",
    "To see this in action, consider the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11347a-165b-4ac0-82f3-b5b11d5b26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_visualize_decision_boundary(classifier, features, labels, title = None):\n",
    "    classifier.fit(features, labels)\n",
    "    visualize_decision_boundary(classifier, features, labels, title = title)\n",
    "    return classifier.score(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea1cf6-3005-449f-a5f2-bd316d18deef",
   "metadata": {},
   "source": [
    "We can use the above function with a classifier and **unscaled** Covid-19 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9727c1f8-62c1-40f8-93ad-27a4c7522fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "# Note that we are using the raw data from `covid_data`,\n",
    "# and not the scaled data from `covid_features`.\n",
    "two_features = covid_data[['titer', 'blood_oxygen']]\n",
    "\n",
    "accuracy = fit_and_visualize_decision_boundary(classifier, two_features, covid_labels, 'No Scaling')\n",
    "print(\"Accuracy without scaling: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d03c9-9592-4125-84f8-08c83b349bed",
   "metadata": {},
   "source": [
    "Now we can do the same thing, but this time use a pipeline that scales the features instead of just a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9606405-91f7-4cc1-b3e3-880f57ae427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('Scale Features', sklearn.preprocessing.StandardScaler()),\n",
    "    ('Predict', sklearn.linear_model.LogisticRegression()),\n",
    "])\n",
    "\n",
    "# Note that we are using the raw data from `covid_data`,\n",
    "# and not the scaled data from `covid_features`.\n",
    "two_features = covid_data[['titer', 'blood_oxygen']]\n",
    "\n",
    "accuracy = fit_and_visualize_decision_boundary(pipeline, two_features, covid_labels, 'With Scaling')\n",
    "print(\"Accuracy without scaling: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd646be4-2eb2-4d70-9e31-957f9d84ed65",
   "metadata": {},
   "source": [
    "We were able to use en entire pipeline in place of a classifier because it uses the same `fit()`/`predict()` methods.\n",
    "Note how we got a higher accuracy and see a different decision boundary with the scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2df23-d6c5-4c51-8724-41714a55ecec",
   "metadata": {},
   "source": [
    "### Test-Train Splits\n",
    "\n",
    "Up until this point, we have mentioned or hinted at the need to set some data aside to test on after we train our machine learning model.\n",
    "In this section, we will finally explicitly discuss this concept.\n",
    "\n",
    "Before we jump into the details, let's imagine a situation that shows why we need to split up our data.\n",
    "Imagine we make a classifier that just remembers every single data point it has ever seen.\n",
    "So on `fit()`, all it does is store each data point along with its label.\n",
    "Then when it comes time to predict, the classifier will just look to see if it has seen the point before.\n",
    "If it has seen the point it returns the real label,\n",
    "and if it has not seen the point it returns a random label.\n",
    "\n",
    "If we train this classifier with `covid_features` like we have with the classifiers we have been working with earlier in this assignment,\n",
    "then this classifier will always score 100% (since it has seen every single point).\n",
    "But as soon as we ask the classifier to predict on new data,\n",
    "it will just give random predictions.\n",
    "So in testing it will do perfect, but in the real world it will perform poorly.\n",
    "\n",
    "In this case it is easy to see that our classifier will never work in the real world,\n",
    "but it is easy to [unintentionally make models that memorize data points](https://bair.berkeley.edu/blog/2019/08/13/memorization/)\n",
    "(especially if you are using neural networks).\n",
    "So to help avoid this, we split our data into multiple non-overlapping parts: [train and test](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets).\n",
    "The *train dataset* (also called \"train set\", \"train split\", or just \"train\") is the data that we will use to train/fit our models.\n",
    "We can also compute our evaluation metrics (e.g. accuracy) on our train split,\n",
    "but the resulting score is just used for debugging purposes.\n",
    "The *test dataset* (also called \"test set\", \"test split\", or just \"test\") is the data that we will officially compute our evaluation metrics on.\n",
    "The test set should never be used for any training purposes and should remain secret until your model is ready to be evaluated.\n",
    "It is considered cheating if information \"leaks\" between the two data splits.\n",
    "\n",
    "<center><img src=\"predictive_models.png\" width=\"500px\"/></center>\n",
    "<center style='font-size: small'>Comic courtesy of <a href='https://xkcd.com/2169/'>xkcd</a>.</center>\n",
    "<center style='font-size: small'>When you don't split your data properly, you may unknowingly leak information.</center>\n",
    "\n",
    "Sometimes, you may even need a third split of data.\n",
    "This third split is called the *validation dataset* (also called \"validation set\", \"validation split\", or just \"validation\").\n",
    "We won't be going into the details of what the validation split is used for,\n",
    "but (if it exists) it is generally the same size as the test split and used to train [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter).\n",
    "Hyperparameters are options that are at a higher level than normal model parameters\n",
    "(like the option of which machine learning model to use).\n",
    "\n",
    "There is no exact number or percent that decides how much of your data should be in train vs test.\n",
    "It just depends on your domain and how much data you have available.\n",
    "If there are no special circumstances, putting aside 10% - 25% of your data for testing typically works well.\n",
    "\n",
    "sklearn has a very simple function available that will split data for you:\n",
    "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "If you have more complex data, splitting by hand should is fairly straightforward\n",
    "(just make sure you don't with skip data points or double include them in both splits).\n",
    "\n",
    "For example, we may decide to hold out 20% of our Covid-19 data for our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4723c7-0007-4438-bef0-025cd332772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broken up over two lines just because the lines are really long.\n",
    "splits = sklearn.model_selection.train_test_split(covid_features, covid_labels, test_size = 0.20)\n",
    "covid_features_train, covid_features_test, covid_labels_train, covid_labels_test = splits\n",
    "\n",
    "# Make sure the sizes match up.\n",
    "assert len(covid_features_train) == len(covid_labels_train)\n",
    "assert len(covid_features_test) == len(covid_labels_test)\n",
    "\n",
    "print(\"Got %d train data points.\" % (len(covid_features_train)))\n",
    "print(\"Got %d test data points.\" % (len(covid_features_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f600f58-5608-4068-8a72-faf79eaef3d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: darkorange\";>★ Task 1.B</h3>\n",
    "\n",
    "Complete the function below that takes in some data (as a dict) and the maximum percentage of the passed in data that should be in the test set.\n",
    "The function should split the passed in data into train and test sets and return them.\n",
    "\n",
    "Both train and test should be formatted as a dict that matches the passed in data\n",
    "(columns names are the keys and the values are the column values).\n",
    "\n",
    "Truncate when computing the exact number of test records.\n",
    "For example if there are 5 records and `test_percentage = 0.5`, then the test set should have 2 records in it.\n",
    "In the cases the `test_percentage` is too low or high,\n",
    "the test set should always have at least one record and at most all except one record\n",
    "(the train set also needs to have at least one record).\n",
    "\n",
    "You may shuffle the resulting splits if you so choose, it will not affect scoring.\n",
    "\n",
    "Note how [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "does not work right away here, since your data is formatted differently.\n",
    "Sometimes you may have to either split your data yourself,\n",
    "or convert it into a format that sklearn.model_selection.train_test_split() understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366e6af-a77b-4222-b076-bbfe9eaf6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dict_data(data, test_percentage):\n",
    "    \"\"\"\n",
    "    Split the passed in data.\n",
    "\n",
    "    Args:\n",
    "        data: The data to split.\n",
    "            It is a dict with column names as keys and column contents as values.\n",
    "        test_percentage: The float percentage of data that should be put into the test split.\n",
    "\n",
    "    Returns:\n",
    "        The training data in the same dict structure as the passed in data.\n",
    "        The test data in the same dict structure as the passed in data.\n",
    "    \"\"\"\n",
    "\n",
    "    return NotImplemented, NotImplemented\n",
    "\n",
    "data = {\n",
    "    'people': [\"Susan\", \"Robin\", \"Avery\", \"Jacob\", \"May\"],\n",
    "    'pets': [\"ostrich\", \"iguana\", \"eagle\", \"dog\", \"rock\"],\n",
    "    'music': [\"rap\", \"psychedelic\", \"country\", \"metal\", \"rock\"],\n",
    "}\n",
    "\n",
    "train, test = split_dict_data(data, test_percentage = 0.50)\n",
    "print(\"Train Data: \", train)\n",
    "print(\"Test Data: \", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dab21f-7ce3-4cc6-a029-000a0c85ec28",
   "metadata": {},
   "source": [
    "# Part 2: Linear Models and Regularization\n",
    "\n",
    "Now that we have taken a quick tour through scikit-learn,\n",
    "let's dive into the theory of the models we have been using.\n",
    "In this section, we will look into linear models and the basic tools we use to train them.\n",
    "\n",
    "Concepts will will explore in this part:\n",
    " - Linear Models\n",
    " - Loss Functions\n",
    " - Gradient Descent\n",
    " - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44758ec5-9b27-4b6f-85a4-ce383a428b64",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "[Linear models](https://scikit-learn.org/stable/modules/linear_model.html)\n",
    "are some of the most common machine learning models to use in practice because they are generally fast, reliable, and easy to interpret.\n",
    "(The linear models we will be discussing are binary by nature (only predict True/False),\n",
    "but as we have discussed earlier any binary classifier can be converted into a multiclass classifier.)\n",
    "\n",
    "Linear models are characterized by using a linear combination of a data point's features to predict a label:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x, b, w) = b + w_1 x_1 + ... + w_p x_p\n",
    "$$\n",
    "\n",
    "Where:\n",
    " - $ \\hat{y} $ is the predicted label.\n",
    " - $ x $ is the features for a single data point.\n",
    " - $ b $ is called a \"bias\", and is any real number.\n",
    " - $ w $ is a vector of weights (also real numbers), one for each feature.\n",
    "   (In different notations, the bias is sometimes included in $ w $ as $ w_0 $.)\n",
    "\n",
    "Together, we refer to the weights and bias as the \"parameters\" of the model,\n",
    "and use the symbol $ \\theta $ to represent them.\n",
    "Notice that the weighted sum can also be written as a dot product:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x, b, w) = b + w \\cdot x\n",
    "$$\n",
    "\n",
    "What this equation is saying, is that a linear model wants to predict a label $ \\hat{y} $\n",
    "by adding together the weighted sum of each feature and the bias.\n",
    "Then a classifier can interpret the result by saying that the more positive this weighted sum is,\n",
    "the more positive label it (and negative values mean a negative label).\n",
    "\n",
    "For example, consider the following data about students studying for a class.\n",
    "\n",
    "| Student | Passes Class (label) | Hours Studied | Taken Class Before |\n",
    "|---------|----------------------|---------------|--------------------|\n",
    "| Alice   | True                 | 40            | False              |\n",
    "| Bob     | True                 | 10            | True               |\n",
    "| Claire  | False                | 10            | False              |\n",
    "\n",
    "Now let's do some feature scaling and replace True/False with 1/-1).\n",
    "\n",
    "| Student | Passes Class (label) | Hours Studied | Taken Class Before |\n",
    "|---------|----------------------|---------------|--------------------|\n",
    "| Alice   |  1                   |  1.4          | -1                 |\n",
    "| Bob     |  1                   | -0.7          |  1                 |\n",
    "| Claire  | -1                   | -0.7          | -1                 |\n",
    "\n",
    "Assuming a bias $ b = 0.0 $ and weights $ w = [1.0, 1.0] $,\n",
    "we will get the following predicted values.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{y}_{alice}  & = 0.0 + 1.0 * \\ \\ \\ 1.4 + 1.0 *      -1.0 & = \\ \\ \\ 1.4 - 1.0 & = \\mathbf{ \\ \\ \\ 0.4} \\\\\n",
    "    \\hat{y}_{bob}    & = 0.0 + 1.0 *      -0.7 + 1.0 * \\ \\ \\ 1.0 & =      -0.7 + 1.0 & = \\mathbf{ \\ \\ \\ 0.3} \\\\\n",
    "    \\hat{y}_{claire} & = 0.0 + 1.0 *      -0.7 + 1.0 *      -1.0 & =      -0.7 - 1.0 & = \\mathbf{      -1.7} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here we can see (even with our dummy weights) that data points with a positive label (Alice and Bob) will get a positive value,\n",
    "while Claire gets a negative value.\n",
    "(If we set $ b $ to $ 0.7 $, then we would have an almost perfect classifier.)\n",
    "With these values, the classifier can set a threshold (recall the thresholds from HO2).\n",
    "For simplicity, let the threshold for this example be $ 0.0 $.\n",
    "So all positive values get assigned to the positive class, and other values get assigned to the negative class.\n",
    "\n",
    "Even with this simple example we can see one of the strengths of linear models,\n",
    "they are easy to interpret.\n",
    "To interpret how a linear model makes decisions, we just have to look at the weights.\n",
    "The sign of the weight tells us which class the feature favors.\n",
    "In our example both feature weights (not the bias) are positive,\n",
    "so the weights favor more studying and taking the class before.\n",
    "Then, weights with low magnitude means the feature **is not** very important,\n",
    "and weights with high magnitude means the feature **is** very important.\n",
    "\n",
    "Normally, we would have to learn the parameters, $ \\theta $, (bias and weights) for a model from training data\n",
    "(remember the thresholds from HO2).\n",
    "This is the job of training/fitting.\n",
    "In this example, we just manually selected the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b941374-c094-4c7d-b026-db897f8d513a",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "A class of models that extend linear models are [general linear model](https://en.wikipedia.org/wiki/General_linear_model).\n",
    "The main difference with general linear models is that they apply a function to the linear combination before making a prediction:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x, b, w) = f( b + w \\cdot x )\n",
    "$$\n",
    "\n",
    "It may seem like a simple change, but it allows for much more flexibility when creating models.\n",
    "\n",
    "On of the most popular and effective linear models is [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).\n",
    "We have used a logistic regression classifier multiple times in this assignment,\n",
    "but we have not yet discussed the theory behind it.\n",
    "Logistic regression is a general linear model that applies the [logistic function](https://en.wikipedia.org/wiki/Logistic_function) to the linear combination.\n",
    "(The logistic function is sometimes also called the \"sigmoid\", \"expit\", or \"inverse logit\" function.)\n",
    "The equation for the standard logistic function is bellow:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "The interesting thing about the logistic function is that is tends to produce values that are close to either $ 0.0 $ or $ 1.0 $.\n",
    "\n",
    "<center><img src=\"logistic.png\" style=\"background-color: white\" width=\"500px\"/></center>\n",
    "<center style='font-size: small'>Image courtesy of <a href='https://en.wikipedia.org/wiki/File:Logistic-curve.svg'>Wikimedia Commons</a>.</center>\n",
    "\n",
    "This makes it ideal for a binary classifier where $ 0.0 $ can represent the negative class, and $ 1.0 $ can represent the positive class.\n",
    "Then our logistic regression classifier can set the classification threshold to $ 0.5 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0bb916-aa8b-4fd7-a588-a291fa7193a2",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "As mentioned earlier,\n",
    "when we fit/train a linear classifier we are really just figuring out the best values for the bias and feature weights.\n",
    "Like in HO2, if we want to train a model, we need a loss function.\n",
    "Recall that a loss function tells us how well our model is performing.\n",
    "Unlike in HO2 where we were just setting a single threshold value (and could therefore use a brute force approach),\n",
    "we now need to set a bias and weight for each feature.\n",
    "\n",
    "There are [many different known loss functions](https://en.wikipedia.org/wiki/Loss_functions_for_classification) that we can choose from.\n",
    "One of the most common types of loss is [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy),\n",
    "also commonly referred to as \"log loss\" or \"logistic loss\".\n",
    "\n",
    "The equation for cross-entropy loss is:\n",
    "$$\n",
    "L_\\text{logistic}(X, Y, \\theta) = - \\sum_{y} \\Pr(Y = y | x, \\theta) \\log \\big( \\Pr(\\hat{Y} = y | x, \\theta) \\big)\n",
    "$$\n",
    "\n",
    "If we let our labels be $ 0 $ and $ 1 $, then we can write it as:\n",
    "$$\n",
    "L_\\text{logistic}(X, Y, \\theta) = - \\sum_{i = 1}^{|Y|} \\bigg[ y_i \\log \\big( \\Pr(\\hat{y}_i = 1 | x_i, \\theta) \\big) + (1 - y_i) \\log \\big( \\Pr(\\hat{y}_i = 0 | x_i, \\theta) \\big) \\bigg]\n",
    "$$\n",
    "\n",
    "This equation may look complex, but we can break it into smaller pieces to see what it is doing.\n",
    "\n",
    "First, we can see that it is summing from $ i = 1 $ to $ |Y| $,\n",
    "so that means it is just summing over every data point.\n",
    "\n",
    "Next, we can see that there are two different parts to the sum, joined by addition:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y_i       &*  \\log \\big( \\Pr(\\hat{y}_i = 1 | x_i, \\theta) \\big) \\\\\n",
    "    (1 - y_i) &*  \\log \\big( \\Pr(\\hat{y}_i = 0 | x_i, \\theta) \\big) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice how one term starts with a $ y_i $ and the other starts with a $ (1 - y_i) $.\n",
    "Since we said earlier that we are setting out labels to either $ 0 $ and $ 1 $,\n",
    "when one of these terms is multiplied by $ 1 $ the other must be multiplied by $ 0 $.\n",
    "Therefore, depending on the actual label, $ y_i $, only one of these terms is active.\n",
    "Specifically, the term that corresponds to the correct label is activated\n",
    "(when $ y_i $ is 1 (positive label) the term with $ \\Pr(\\hat{y}_i = 1 | x_i, \\theta) $ is active and vice versa).\n",
    "\n",
    "If the label is true ($ 1 $), then the top term is active and can be simplified to:\n",
    "\n",
    "$$\n",
    "\\log \\big( \\Pr(\\hat{y}_i = 1 | x_i, \\theta) \\big)\n",
    "$$\n",
    "\n",
    "Which is just the logarithm of the probability that $ y_i $ is 1 (true).\n",
    "\n",
    "So in the end, all we need to do to compute the cross-entropy (logistic) loss is to\n",
    "find the probabilities of predicting each data point correctly,\n",
    "take the logarithm of those probabilities,\n",
    "sum them,\n",
    "and negate the sum.\n",
    "\n",
    "(\n",
    "If you are having trouble parsing equations, then try to take the approach we went through above.\n",
    "Understanding equations can be hard, so just take your time and try to figure out what each piece of the equation is trying to do.\n",
    "Then see if you can piece it together and figure out what the goal of the equation is.\n",
    ")\n",
    "\n",
    "Now we have a mathematical model for our classifier and a loss function,\n",
    "but how actually do the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c807fc-1e6e-4486-afcd-041e0ab25113",
   "metadata": {},
   "source": [
    "## Gradient Descent (GD)\n",
    "\n",
    "To train our classifier (learn $ \\theta $ ($ w $ and $ b $ for a linear model)),\n",
    "we need to be able to minimize the loss function.\n",
    "For example, we may need to solve (choose the $ \\theta $ that minimizes the loss):\n",
    "\n",
    "$$\n",
    "\\underset{\\theta}{\\mathrm{argmin}} \\ L_\\text{logistic}(X, Y, \\theta)\n",
    "$$\n",
    "\n",
    "One of the simplest and most effective means of training machine learning models is with\n",
    "[gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "The main idea behind gradient descent is simple,\n",
    "use the gradient of the loss to point us in the right direction and take a small step in that direction.\n",
    "We can slowly move our parameters (weights and bias) to follow the local gradients to find a local minima for loss.\n",
    "\n",
    "Intuitively, we can think of gradient descent as sliding downhill in the steepest possible direction,\n",
    "as in the below image.\n",
    "\n",
    "<center><img src=\"gradient_descent.gif\" style=\"background-color: white\" width=\"500px\"/></center>\n",
    "<center style='font-size: small'>Image courtesy of <a href='https://angeloyeo.github.io/2020/08/16/gradient_descent.html'>angeloyeo.github.io</a>.</center>\n",
    "\n",
    "You may be wondering why we take many small steps instead of just solving the minimization in one step\n",
    "(like you may have done in a calculus class).\n",
    "If we know what the exact form of your loss function (which you will not always know)\n",
    "and it has a closed and differentiable form (which it will not always),\n",
    "then it can be tempting to just solve it directly.\n",
    "But remember that we are not typically working with loss functions that have only one or two variables.\n",
    "The Covid-19 data we are using has 9 dimensions, and it is considered very small.\n",
    "In real world scenarios you can easily have hundreds, thousands, or even more dimensions.\n",
    "So it is usually faster to solve iteratively using something like gradient descent instead of directly.\n",
    "(However, sometimes we can take shortcuts by using insights from a loss' closed form.)\n",
    "\n",
    "Gradient descent is simple, but has shown to be very powerful for machine learning.\n",
    "Many of the most effective fitting algorithms we use today are based on gradient descent.\n",
    "Of those, [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD)\n",
    "is probably the most successful variant (that has spawned its own family of derived methods).\n",
    "We will not go into detail of SGD in this assignment,\n",
    "but we will mention it since you will be using it in this assignment\n",
    "(and are likely to run into it in the wild).\n",
    "Instead of looking at all points at once, SGD chooses a random batch of data points and just computes a gradient over them.\n",
    "It would seem like working with less points at a time would be worse, but SGD is almost always better.\n",
    "\n",
    "Of course, sklearn has its own SGD-based classifier that we can use:\n",
    "[sklearn.linear_model.SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698e446-daad-4e11-a223-4be3cf1edf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an SGD classifier.\n",
    "classifier = sklearn.linear_model.SGDClassifier()\n",
    "\n",
    "two_features = covid_features[['titer', 'blood_oxygen']]\n",
    "\n",
    "classifier.fit(two_features, covid_labels)\n",
    "visualize_decision_boundary(classifier, two_features, covid_labels, \"SGD: Titer vs Blood Oxygen\")\n",
    "\n",
    "print(\"Accuracy: \", classifier.score(two_features, covid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5d385-8843-4a39-9237-ac0e7be511e6",
   "metadata": {},
   "source": [
    "### Experiments with Different Loss Functions\n",
    "\n",
    "Now, we want to explore different loss functions and see how they will perform.\n",
    "We don't have enough time to go into detail of them all,\n",
    "but we can at least take a quick look at how they generate different decision boundaries.\n",
    "However, this is hard to do with our Covid-19 dataset\n",
    "because of the number of data points and complexity of the data.\n",
    "So instead, let's make some simple synthetic classification data using sklearn's\n",
    "[sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6a44c-72be-4d8b-8172-f01de2f265f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_toy_data(size):\n",
    "    features, labels = sklearn.datasets.make_classification(\n",
    "        n_samples = size, n_features = 2,\n",
    "        n_informative = 2, n_redundant = 0, n_repeated = 0,\n",
    "        random_state = 0,\n",
    "    )\n",
    "\n",
    "    column1, column2 = zip(features.transpose())\n",
    "\n",
    "    features = pandas.DataFrame.from_dict({\"a\": column1[0], \"b\": column2[0]})\n",
    "    labels = pandas.Series(labels)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "toy_features, toy_labels = make_toy_data(15)\n",
    "toy_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17443e-7e41-4bb4-bce7-51a3abd9ced1",
   "metadata": {},
   "source": [
    "Let's make a simple function to test out an SGD classifier with different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dea26e-9fd7-4fa6-a4ba-a493d57bb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_with_loss(features, labels, loss):\n",
    "    classifier = sklearn.linear_model.SGDClassifier(loss = loss, penalty = None,\n",
    "                                                    max_iter = 100, random_state = 0)\n",
    "\n",
    "    title = \"SGD with %s Loss\" % (loss)\n",
    "    accuracy = fit_and_visualize_decision_boundary(classifier, features, labels, title)\n",
    "\n",
    "    print(\"Accuracy for %s loss: %f.\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969286f-4aad-4b1b-b387-d00a1fe66ae7",
   "metadata": {},
   "source": [
    "Now that we have some simple data and a testing function,\n",
    "we can see how different loss functions create different classifiers (and decision boundaries).\n",
    "\n",
    "We have already discussed cross-entropy loss in this assignment,\n",
    "and you have already discussed L1 and L2 losses in class.\n",
    "Below, we will invoke several different loss function that you are not responsible for learning.\n",
    "By looking at the different decision boundaries created by these different loss functions,\n",
    "we hope to show you how different parameters of a classifier (the loss function in this case) can create very different decision boundaries.\n",
    "And even though the decision boundaries may be different, they may still result in similar scores in the end.\n",
    "If do you want to read about the details of each loss function,\n",
    "you can start with the [sklearn loss documentation](https://scikit-learn.org/stable/modules/sgd.html#sgd-mathematical-formulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3baba-07e4-48f8-82f4-5ee40bf836dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_with_loss(toy_features, toy_labels, 'log_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f863b3-bc08-42d1-ac37-936bdb92bd65",
   "metadata": {},
   "source": [
    "We have already seen `log_loss` once before.\n",
    "Notice here that this classifier only achieved an accuracy of 0.87,\n",
    "so that one red point on the decision boundary actually falls on the blue side and is misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7467d30d-9893-4530-84b7-77daf51b1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_with_loss(toy_features, toy_labels, 'hinge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1141a3c-9e44-4e25-b058-03a0c340d3a2",
   "metadata": {},
   "source": [
    "Using the `hinge` loss we see the same accuracy as the `log_loss`,\n",
    "but a different decision boundary is created.\n",
    "This one is steeper and misclassifies a different red point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14dd471-d45b-480d-93b8-1056c498f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_with_loss(toy_features, toy_labels, 'perceptron')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030eed0e-0231-4dc1-86ad-bfd1c941f3fb",
   "metadata": {},
   "source": [
    "Here we can see the `perceptron` loss performed significantly worse\n",
    "and chose a very different decision boundary from the other loss functions.\n",
    "In this case, our classifier produced no false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9fea52-3522-4cf9-a82d-50b2abfcd754",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_with_loss(toy_features, toy_labels, 'modified_huber')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a090690-a64d-4dc8-b13e-89978fba663f",
   "metadata": {},
   "source": [
    "The `modified_huber` loss performed just as poorly as the `perceptron` loss in this case,\n",
    "but found a very different decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec2a99-36d8-4861-b9a4-356e0308f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_with_loss(toy_features, toy_labels, 'squared_hinge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719a119-6c84-443e-8d6b-35a635700d1e",
   "metadata": {},
   "source": [
    "The `squared_hinge` loss seems to perform the best in this case,\n",
    "only misclassifying one point.\n",
    "\n",
    "Note that we have not explored these different loss functions enough to draw any conclusions or intuitions.\n",
    "However, we can see that different loss functions do affect how a classifier performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3e8ad-3db1-4699-b1b2-cfa68e9db36a",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "What happens when we are trying to train a classifier,\n",
    "but there are many different parameters that give the same loss?\n",
    "Think back to our student studying example.\n",
    "We choose the $ w = [1.0, 1.0 ] $,\n",
    "but what if we instead used $ w = [100.0, 100.0 ] $?\n",
    "We would end up with the same loss (zero because we classified everything perfectly),\n",
    "but we would end up with weights that are 100 times larger in magnitude.\n",
    "Which weights should our classifier prefer, $ 1.0 $ or $ 100.0 $?\n",
    "\n",
    "To decide cases like this, we use [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)).\n",
    "Regularization can be used for a variety of situations,\n",
    "the most common ones is to punish models for being too complex and enforce constraints.\n",
    "In this assignment, we will cover the former (punishing complex models).\n",
    "\n",
    "We we talk about \"complex\" linear models, we usually mean models with many large (and non-zero) weights.\n",
    "In terms of complexity, nothing is simpler than nothing,\n",
    "so we can use regularization to tell our model to prefer weights closer to zero.\n",
    "This would mean that we would like our model to ignore features that are not very useful.\n",
    "(We may even be able to remove these features from out data if we see them consistently getting zero (or close to zero) weights).\n",
    "\n",
    "The most simple regularizer is the [L1 norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Taxicab_norm_or_Manhattan_norm)\n",
    "(also called the \"Taxicab\" or \"Manhattan\" norm).\n",
    "The L1 norm of $ \\theta $ (written as $ || \\theta ||_1 $) is just sums the absolute value of $ \\theta $\n",
    "(which is also the [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry) between the origin and $ \\theta $).\n",
    "\n",
    "$$\n",
    "|| \\theta ||_1 = \\sum_i | \\theta_i |\n",
    "$$\n",
    "\n",
    "We can just add our regularizer to our loss to create our new regularized/total loss\n",
    "(along with an $ \\alpha $ hyperparameter that controls how influential the regularizer is):\n",
    "\n",
    "$$\n",
    "L_\\text{total}(X, Y, \\theta) = L_\\text{logistic}(X, Y, \\theta) + \\alpha || \\theta ||_1\n",
    "$$\n",
    "\n",
    "Now if we consider our student example again,\n",
    "we can see that we will get different losses because of the regularizer:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    L_\\text{total}(X, Y, \\theta = {b = 0.0, w = [1.0,   1.0  ]}) & = 0.0 + || \\theta ||_1 & = |0.0| + |1.0|   + |1.0|   & = \\mathbf{2.0}   \\\\\n",
    "    L_\\text{total}(X, Y, \\theta = {b = 0.0, w = [100.0, 100.0]}) & = 0.0 + || \\theta ||_1 & = |0.0| + |100.0| + |100.0| & = \\mathbf{200.0} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It is clear that with our regularizer, the lower weights are preferred (have a lower loss).\n",
    "\n",
    "\n",
    "Probably the most popular norm and sibling of the L1 norm is the [L2 norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm)\n",
    "(also called the \"Euclidean\" norm).\n",
    "The L2 norm of $ \\theta $ (written as $ || \\theta ||_2 $) is the square root of the sum of squares of $ \\theta $\n",
    "(which is also the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) between the origin and $ \\theta $).\n",
    "\n",
    "$$\n",
    "|| \\theta ||_2 = \\sqrt{\\sum_i \\theta_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfa597-3cac-419c-a583-6c0453390c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = 'l1'\n",
    "\n",
    "# Make an SGD classifier with an L1 penalty and 0.1 alpha.\n",
    "classifier = sklearn.linear_model.SGDClassifier(loss = 'hinge',\n",
    "                                                penalty = regularizer, alpha = 0.1,\n",
    "                                                max_iter = 100, random_state = 1)\n",
    "\n",
    "title = \"SGD with %s Regularizer\" % (regularizer)\n",
    "accuracy = fit_and_visualize_decision_boundary(classifier, toy_features, toy_labels, title)\n",
    "\n",
    "print(\"Accuracy for %s regularizer: %f.\" % (regularizer, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9674764-2314-4727-81c7-ea5e74a54c2d",
   "metadata": {},
   "source": [
    "Using the L1 regularizer produced a different decision boundary than in our previous test with the `hinge` loss,\n",
    "but gets the same accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd83bab-112d-49b9-aa64-b53f67d7a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = 'l2'\n",
    "\n",
    "# Make an SGD classifier with an L1 penalty and 0.1 alpha.\n",
    "classifier = sklearn.linear_model.SGDClassifier(loss = 'hinge',\n",
    "                                                penalty = regularizer, alpha = 0.1,\n",
    "                                                max_iter = 100, random_state = 1)\n",
    "\n",
    "title = \"SGD with %s Regularizer\" % (regularizer)\n",
    "accuracy = fit_and_visualize_decision_boundary(classifier, toy_features, toy_labels, title)\n",
    "\n",
    "print(\"Accuracy for %s regularizer: %f.\" % (regularizer, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff95340-9af9-41c1-b43d-4ff102c62844",
   "metadata": {},
   "source": [
    "Using the L2 regularizer gives a boost to our accuracy in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d1edf-3a7a-4902-9d53-ef594f041418",
   "metadata": {},
   "source": [
    "# Part 3: Non-Linear Models\n",
    "\n",
    "All the classifiers we have used so far in this assignment have been linear models, i.e.,\n",
    "they create a linear decision boundary.\n",
    "\n",
    "The simplicity of linear models make them fast to train and easy to interpret.\n",
    "But, their simplicity also means there will be some data sets that are impossible to correctly classify.\n",
    "Take another look at the toy data set we used in the last part.\n",
    "No matter how we orient our decision boundary,\n",
    "it is impossible to separate all the blue and red points.\n",
    "\n",
    "Non-linear models flip the pros and cons of linear models.\n",
    "Non-linear models may be more complex and therefore more computationally costly and harder to interpret,\n",
    "but a non-linear decision boundary can split data that could not be cleanly split with a linear decision boundary.\n",
    "\n",
    "In this part, we will discuss a few non-linear machine learning models:\n",
    " - [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    " - [K-Nearest Neighbors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25e2a2-e009-46ef-a88d-a61a6e539128",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "[Decision trees](https://en.wikipedia.org/wiki/Decision_tree) are a class of models that attempt to create a tree\n",
    "where each non-leaf node splits the data (usually based on a single feature).\n",
    "For example, consider the following decision tree that predicts whether a passenger on the Titanic survived its sinking.\n",
    "\n",
    "<center><img src=\"decision_tree.jpg\" style=\"background-color: white\" width=\"500px\"/></center>\n",
    "<center style='font-size: small'>\n",
    "    A tree showing survival of passengers on the Titanic\n",
    "    (\"sibsp\" is the number of spouses or siblings aboard).\n",
    "    <br />\n",
    "    The figures under the leaves show the probability of survival and the percentage of observations in the leaf.\n",
    "    <br />\n",
    "    Summarizing: Your chances of survival were good if you were (i) a female or (ii) a male at most 9.5 years old with strictly fewer than 3 siblings.\n",
    "</center>\n",
    "<center style='font-size: small'>Image courtesy of <a href='https://en.wikipedia.org/wiki/File:Decision_Tree.jpg'>Wikimedia Commons</a>.</center>\n",
    "\n",
    "Although this tree only makes binary splits and uses a single variable at a time,\n",
    "it is possible to create trees that have nodes with more than two children\n",
    "and nodes that consider more than one feature at a time.\n",
    "\n",
    "The biggest benefit of decision tress is that they are interpretable (as long as the tree is relatively small).\n",
    "Decision trees are a natural way for people to process data,\n",
    "and are even used outside of machine learning to help normal non-electric humans make decisions.\n",
    "Some drawbacks of decision trees is that they are sensitive to changes in the data\n",
    "(small changes in the data can result in generating entirely different trees)\n",
    "and they can be much harder to learn than linear models.\n",
    "\n",
    "A common way to use decision tree is to create several tress and combine them into a [random forest](https://en.wikipedia.org/wiki/Random_forest).\n",
    "Random forests are an [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) technique that has multiple decision trees\n",
    "that all get to vote on what the prediction should be.\n",
    "Using multiple trees allows different parameters (or even algorithms) to be used to generate each tree,\n",
    "so the overall models gets to look at the data in several different ways.\n",
    "\n",
    "<center><img src=\"random_forest.png\" style=\"background-color: white\" width=\"500px\"/></center>\n",
    "<center style='font-size: small'>Image courtesy of <a href='https://en.wikipedia.org/wiki/File:Random_forest_diagram_complete.png'>Wikimedia Commons</a>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a203e-2511-42e7-8cd9-2b18967310f4",
   "metadata": {},
   "source": [
    "Let's try using a decision tree to classify our toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9914b6-c7af-4c18-af1c-d352e1085b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a decision tree classifier a max depth of 3.\n",
    "decision_tree = sklearn.tree.DecisionTreeClassifier(max_depth = 3, random_state = 0)\n",
    "\n",
    "title = \"Decision Tree with Max Depth of 3\"\n",
    "accuracy = fit_and_visualize_decision_boundary(decision_tree, toy_features, toy_labels, title)\n",
    "\n",
    "print(\"Accuracy: %f.\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219cfd3-db86-49e0-b24b-8a2db86e6f98",
   "metadata": {},
   "source": [
    "In addition to visualizing the decision boundary of this tree,\n",
    "we can also visualize the tree itself.\n",
    "Child nodes on the left represent data points where the condition in the parent node is true,\n",
    "while child nodes on the right represent data points where the condition in the parent is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2accefd4-b587-43a8-a0d7-52e90f1d9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.tree.plot_tree(decision_tree, feature_names = ['a', 'b'], class_names = ['0', '1'],\n",
    "                       filled = True, impurity = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f1394-4f7c-474a-86da-e080cd926090",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange\";>★ Task 3.A</h3>\n",
    "\n",
    "Complete the function below, which re-implements the same decision tree as above using if statements.\n",
    "The function's argument are the features for a single data point,\n",
    "and the function should return 0 or 1 (as shown in the above tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa00a0-3689-4219-be7d-85efc758349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_decision_tree(features):\n",
    "    \"\"\"\n",
    "    Manually re-create a decision tree.\n",
    "\n",
    "    Args:\n",
    "        features: A list of floats that represents the features for a single data point.\n",
    "\n",
    "    Returns:\n",
    "        The prdeicted label as an int.\n",
    "    \"\"\"\n",
    "\n",
    "    return NotImplemented\n",
    "\n",
    "test_point = [0.0, 0.0]\n",
    "print(\"Label of test point (%s): %s\" % (test_point, manual_decision_tree(test_point)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b602eee0-64a8-41d6-bd33-d4cf99735fef",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "Our next non-linear model is [K-Nearest Neighbors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\n",
    "KNN is a very simple an intuitive model.\n",
    "During training, KNN just remembers the data points and labels provided.\n",
    "Then for each data point KNN is to make a prediction for,\n",
    "the classifier finds the K nearest training data points\n",
    "and predicts the most common label amongst those nearest labels.\n",
    "\n",
    "The following diagram shows an example of KNN.\n",
    "\n",
    "<center><img src=\"knn.png\" style=\"background-color: white\" width=\"500px\"/></center>\n",
    "<center style='font-size: small'>\n",
    "    Example of k-NN classification.\n",
    "    <br />\n",
    "    The test sample (green dot) should be classified either to blue squares or to red triangles.\n",
    "    <br />\n",
    "    If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle.\n",
    "    <br />\n",
    "    If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).\n",
    "</center>\n",
    "<center style='font-size: small'>Image courtesy of <a href='https://en.wikipedia.org/wiki/File:KnnClassification.svg'>Wikimedia Commons</a>.</center>\n",
    "\n",
    "The benefits of KNN is that it is\n",
    "fast to train (it just needs to store the data points);\n",
    "lazy (it only does computation work when it actually needs to, like any good Computer Science student);\n",
    "and easy to interpret (to explain why a prediction is made, KNN just needs to provide the similar training data points).\n",
    "\n",
    "The drawbacks of KNN is that it\n",
    "has to search against all known data points when making predictions (smart implementation can do some pre-computation to minimize this time, but many comparisons must still be made);\n",
    "runs into trouble if the class labels are imbalanced;\n",
    "and uses much more memory than most models (KNN needs to store all training data, but linear models only need to store a bias and weights).\n",
    "\n",
    "Let visualize a KNN classifier where K = 1 (a 1-NN classifier),\n",
    "which classifies new data using the same label as the closest point to it in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef200fd-aedd-4296-a75d-d8ab19a57ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a KNN classifier with a K of 1.\n",
    "classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "title = \"KNN with K = 1\"\n",
    "accuracy = fit_and_visualize_decision_boundary(classifier, toy_features, toy_labels, title)\n",
    "\n",
    "# Accuracy will always be 100% since we are memorizing all the data points.\n",
    "print(\"Accuracy: %f.\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a08e6-a9ea-4a9c-af96-b63e94255a93",
   "metadata": {},
   "source": [
    "Of course our 1-NN classifier gets 100% accuracy, since it memorized all the training data (and we are not splitting our toy data).\n",
    "If we use our Covid-19 splits we can see KNN getting the expected 100% accuracy when scored on the train data,\n",
    "and a lower score on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f038e-cf96-4710-ba47-bb65f9ee8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "classifier.fit(covid_features_train, covid_labels_train)\n",
    "\n",
    "print(\"Train Score: \", classifier.score(covid_features_train, covid_labels_train))\n",
    "print(\"Test Score: \", classifier.score(covid_features_test, covid_labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efdcd5a-a636-4994-90b4-93576d2d5b42",
   "metadata": {},
   "source": [
    "What does our decision boundary look like if we revisit the toy data with a K of 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13669ee-c7e6-46fd-ae74-5ae10738d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a KNN classifier with a K of 3.\n",
    "classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "title = \"KNN with K = 3\"\n",
    "fit_and_visualize_decision_boundary(classifier, toy_features, toy_labels, title)\n",
    "\n",
    "print(\"Accuracy: %f.\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e28cc-5830-42fd-83da-8d70bd6bf50e",
   "metadata": {},
   "source": [
    "With a K of 3, KNN doesn't score 100% on the training data.\n",
    "Note that this does not mean KNN is doing poorly.\n",
    "It just means that 1-NN is a special case.\n",
    "\n",
    "Observe how a 3-NN classifier scores better on the Covid-19 test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09a706-ba01-4099-a37b-1eb84030f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "classifier.fit(covid_features_train, covid_labels_train)\n",
    "\n",
    "print(\"Train Score: \", classifier.score(covid_features_train, covid_labels_train))\n",
    "print(\"Test Score: \", classifier.score(covid_features_test, covid_labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b122c-18f8-4c44-9ef3-ab956a583661",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkorange\";>★ Task 3.B</h3>\n",
    "\n",
    "Complete the class below which implements a KNN classifier (and meets the requirements for being a sklearn predictor).\n",
    "You only need to complete the `fit()` and `predict()` methods.\n",
    "You may modify the contents of any methods as you see fit (but not the signatures),\n",
    "and you may make any other functions you want (but prefix any additional functions with an underscore to avoid conflicts with sklearn).\n",
    "\n",
    "Details:\n",
    " - K will always be less than the number of training points.\n",
    " - All features will be numeric.\n",
    " - The number of features will be >= 2.\n",
    " - Labels can be anything hashable (so they can go in a dict or set).\n",
    " - Use euclidean distance to compute distance between points.\n",
    " - In the event of a tie, you may choose which of the tied labels to predict.\n",
    " - `predict()` must return a numpy.ndarray.\n",
    " - You may not use any sklearn functionality aside from the uses already provided (`sklearn.base.BaseEstimator` and `sklearn.metrics.accuracy_score`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e95df-f16c-47cb-a4f0-9b8b9ab9e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKNN(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.ignore_ = True  # Students can ignore this.\n",
    "\n",
    "    def fit(self, train_features, train_labels):\n",
    "        \"\"\"\n",
    "        Train the KNN classifier.\n",
    "\n",
    "        Args:\n",
    "            train_features: A pandas.DataFrame that only contains numeric data.\n",
    "            train_labels: A pandas.Series containing the labels.\n",
    "        \"\"\"\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, test_features):\n",
    "        \"\"\"\n",
    "        Make predictions on the passed in data points.\n",
    "\n",
    "        Args:\n",
    "            test_features: A pandas.DataFrame that only contains numeric data.\n",
    "\n",
    "        Returns:\n",
    "            A numpy.ndarray with the predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        return NotImplemented\n",
    "\n",
    "    def score(self, test_features, test_labels):\n",
    "        predictions = self.predict(test_features)\n",
    "        return sklearn.metrics.accuracy_score(predictions, test_labels)\n",
    "\n",
    "classifier = MyKNN(3)\n",
    "accuracy = fit_and_visualize_decision_boundary(classifier, toy_features, toy_labels, \"MyKNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48cdf59-f82d-4019-95e7-521f2802d3f9",
   "metadata": {},
   "source": [
    "# Part 4: Classic Results\n",
    "\n",
    "A big part of becoming a true data scientist or machine learner is about trying many different things and seeing many different situations.\n",
    "These efforts help to build the intuition that will guide us as we clean data, create models, and tune parameters.\n",
    "In this section, we are going to highlight two pieces of machine learning wisdom:\n",
    " - More Data Produces Better Models\n",
    " - Complex Models May Backfire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3573717-7b1f-4d67-8b8c-895a22045cfc",
   "metadata": {},
   "source": [
    "## More Data Produces Better Models\n",
    "\n",
    "As machine learners, data is at the core of everything we do.\n",
    "So of course, we always want more data.\n",
    "But does more data actually produce better results?\n",
    "Generally ... yes it does.\n",
    "More data gives us more opportunities to see patterns and trends.\n",
    "\n",
    "It is important to note that we are not just talking about any data.\n",
    "We always want to see more labeled (so we can train on it)\n",
    "and trustworthy (so it doesn't trick our models).\n",
    "We call low quality data \"garbage\",\n",
    "and a key tenant of machine learning and data science is:\n",
    "\"Garbage in, garbage out\".\n",
    "This phrase is so important, that is [has it's own Wikipedia page](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out)\n",
    "(for a phrase!).\n",
    "\n",
    "In the below cell, we will see how the training and test accuracy change as more and more data points are given in the training set.\n",
    "We will be specifically looking at a type of graph called a [learning curve](https://en.wikipedia.org/wiki/Learning_curve),\n",
    "which shows how our model improves over time (or in this case, as more data is provided for training on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab998e5e-6ec4-4d0f-9ca8-c67d45f30b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_large_features, toy_large_labels = make_toy_data(5000)\n",
    "classifier = sklearn.tree.DecisionTreeClassifier(max_depth = 5, random_state = 0)\n",
    "\n",
    "figure, axis = matplotlib.pyplot.subplots(1, 1, figsize = (6, 6))\n",
    "axis.set_title(\"Learning Curve for Decision Tree\")\n",
    "\n",
    "sklearn.model_selection.LearningCurveDisplay.from_estimator(\n",
    "    classifier, toy_large_features, toy_large_labels,\n",
    "    cv = sklearn.model_selection.ShuffleSplit(n_splits = 50, test_size = 0.5, random_state = 0),\n",
    "    score_type=\"both\", score_name=\"Accuracy\", ax = axis,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ff2c6-a59b-4cc1-97b1-bdb6d1371823",
   "metadata": {},
   "source": [
    "We see that as more data is provided,\n",
    "the training accuracy decreases and then flattens out.\n",
    "A lower accuracy may seem alarming at first, but this is exactly what we expect to see.\n",
    "As the training set grows larger,\n",
    "our model is moving away from memorizing data points and moving towards learning patterns in the data.\n",
    "These patterns are what can be effectively applied to the test data,\n",
    "and that is why the test accuracy improves as more training data is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2462d6-c321-40fc-996c-3ec81a49cc70",
   "metadata": {},
   "source": [
    "## Complex Models May Backfire\n",
    "\n",
    "Next, we show increasing the depth of the tree only helps testing accuracy up to a point,\n",
    "beyond which we [overfit](https://en.wikipedia.org/wiki/Overfitting) on the training data.\n",
    "Overfitting is a huge challenge, especially with some types of models (like decision trees and neural networks).\n",
    "\n",
    "A model that has overfit has learned the specific peculiarities of its training data,\n",
    "rather than the general patterns of any data drawn from the same distribution.\n",
    "One way to conceptualize overfitting is to think of it as \"memorizing the features of noise\".\n",
    "Generally, a model is more able to overfit when it is\n",
    "- more complex.\n",
    "- trained for longer.\n",
    "\n",
    "Below, we show that increasing model complexity (e.g., the depth of a decision tree),\n",
    "while uniformly associated with higher training accuracy,\n",
    "ultimately allows overfitting to the training data,\n",
    "at which point the model cannot generalize to unseen test data\n",
    "(because it expects it to look too much the training data!).\n",
    "The best performance on the testing data occurs somewhere between a model that is:\n",
    "- too simple to learn anything.\n",
    "- too complex and able to effectively \"memorize\" its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36066143-bbf9-4281-bb9e-9af3a3ea0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tree_depth = 4\n",
    "max_tree_depth = 20\n",
    "\n",
    "figure, axis = matplotlib.pyplot.subplots(1, 1, figsize = (6, 6))\n",
    "axis.set_title(\"Performance vs Model Complexity\")\n",
    "axis.set_xlabel(\"Max Tree Depth\")\n",
    "axis.set_ylabel(\"Accuracy\")\n",
    "\n",
    "max_depths = list(range(min_tree_depth, max_tree_depth))\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    classifier = sklearn.tree.DecisionTreeClassifier(max_depth = max_depth, random_state = 0)\n",
    "    classifier.fit(covid_features_train, covid_labels_train)\n",
    "\n",
    "    train_accuracy = classifier.score(covid_features_train, covid_labels_train)\n",
    "    test_accuracy = classifier.score(covid_features_test, covid_labels_test)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "axis.plot(max_depths, train_accuracies, label = \"Train Accuracy\")\n",
    "axis.plot(max_depths, test_accuracies, label = \"Test Accuracy\")\n",
    "axis.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a8065-a522-47a2-8d75-5174752f89d3",
   "metadata": {},
   "source": [
    "As our model becomes more complex\n",
    "(our tree is allowed to become larger),\n",
    "if first hits a point were the highest test accuracy is achieved.\n",
    "But instead of flattening out (like we saw in our learning curve graph above),\n",
    "the overly complex model starts to score worse on the test data.\n",
    "The training score reaches 100% because the tree has enough complexity to model every point in the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
